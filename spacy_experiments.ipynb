{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries and Packages\n",
    "import sys  \n",
    "import csv\n",
    "import spacy\n",
    "from time import process_time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Set global variables\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500  movies processed\n",
      "1000  movies processed\n",
      "1500  movies processed\n",
      "2000  movies processed\n",
      "2500  movies processed\n",
      "3000  movies processed\n",
      "3500  movies processed\n",
      "4000  movies processed\n",
      "4500  movies processed\n",
      "5000  movies processed\n",
      "5500  movies processed\n",
      "6000  movies processed\n",
      "6500  movies processed\n",
      "7000  movies processed\n",
      "7500  movies processed\n",
      "8000  movies processed\n",
      "8500  movies processed\n",
      "9000  movies processed\n",
      "9500  movies processed\n",
      "10000  movies processed\n",
      "10500  movies processed\n",
      "11000  movies processed\n",
      "11500  movies processed\n",
      "12000  movies processed\n",
      "12500  movies processed\n",
      "13000  movies processed\n",
      "13500  movies processed\n",
      "14000  movies processed\n",
      "14500  movies processed\n",
      "15000  movies processed\n",
      "15500  movies processed\n",
      "16000  movies processed\n",
      "16500  movies processed\n",
      "17000  movies processed\n",
      "17500  movies processed\n",
      "18000  movies processed\n",
      "18500  movies processed\n",
      "19000  movies processed\n",
      "19500  movies processed\n",
      "20000  movies processed\n",
      "20500  movies processed\n",
      "21000  movies processed\n",
      "21500  movies processed\n",
      "22000  movies processed\n",
      "22500  movies processed\n",
      "23000  movies processed\n",
      "23500  movies processed\n",
      "24000  movies processed\n",
      "24500  movies processed\n",
      "25000  movies processed\n",
      "25500  movies processed\n",
      "26000  movies processed\n",
      "26500  movies processed\n",
      "27000  movies processed\n",
      "27500  movies processed\n",
      "28000  movies processed\n",
      "28500  movies processed\n",
      "29000  movies processed\n",
      "29500  movies processed\n",
      "30000  movies processed\n",
      "30500  movies processed\n",
      "31000  movies processed\n",
      "31500  movies processed\n",
      "32000  movies processed\n",
      "32500  movies processed\n",
      "33000  movies processed\n",
      "33500  movies processed\n",
      "34000  movies processed\n",
      "34500  movies processed\n",
      "Processing Done\n",
      "Time to process  34886  records:  2524.3125\n",
      "Average processing timer per record:  0.0723588975520266\n",
      "('Should a Woman Divorce?', Grace Roberts (played by Lea Leland), marries rancher Edward Smith, who is revealed to be a neglectful, vice-ridden spouse. They have a daughter, Vivian. Dr. Franklin (Leonid Samoloff) whisks Grace away from this unhappy life, and they move to New York under aliases, pretending to be married (since surely Smith would not agree to a divorce). Grace and Franklin have a son, Walter (Milton S. Gould). Vivian gets sick, however, and Grace and Franklin return to save her. Somehow this reunion, as Smith had assumed Grace to be dead, causes the death of Franklin. This plot device frees Grace to return to her father's farm with both children.[1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if debug:\n",
    "    start_time = process_time()\n",
    "\n",
    "# Set the appropriate input file\n",
    "# movie_plots_import_file = \"wiki_movie_plots_sample.csv\" # 3248 Movies\n",
    "movie_plots_import_file = \"wiki_movie_plots_deduped.csv\" #34,892 Movies\n",
    "\n",
    "\n",
    "# Setup the plots array\n",
    "plots = []\n",
    "\n",
    "# Open import file and export all plots and titles into plots[]\n",
    "with open(movie_plots_import_file, 'r', encoding='utf-8') as movie_plot_csv_file:\n",
    "    reader = csv.reader(movie_plot_csv_file)\n",
    "    next(reader, None)  # Skip the csv header row\n",
    "    count=0\n",
    "\n",
    "    for row in reader:\n",
    "        plots.append((row[1],nlp(row[7])))\n",
    "        count = count + 1\n",
    "        if(count % 500 == 0):\n",
    "            print(count, \" movies processed\")\n",
    "            \n",
    "print(\"Processing Done\")\n",
    "\n",
    "if debug:\n",
    "    end_time = process_time()\n",
    "    print(\"Time to process \", count, \" records: \", end_time - start_time)\n",
    "    print(\"Average processing timer per record: \", (end_time- start_time)/count)\n",
    "\n",
    "# Print example when finished\n",
    "print(plots[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic similarity function\n",
    "def simularity_printer(firstId, secondId):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    print(\"Similarity of \", plots[firstId][0], \" and \", plots[secondId][0], \":\", plots[firstId][1].similarity(plots[secondId][1]))\n",
    "    if debug:\n",
    "        end_time = process_time()\n",
    "        print(\"Simularity calculation runtime \", end_time - start_time)\n",
    "\n",
    "# Define generic 'best match' function\n",
    "def best_match(id):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    best_score = 0\n",
    "    best_match = \"None\"\n",
    "    for plot in plots:\n",
    "        if plot == plots[id]:\n",
    "            continue\n",
    "        score = plots[id][1].similarity(plot[1])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = plot[0]\n",
    "    print(\"Best match for \",plots[id][0],\" was: \",best_match,\" with score of \", best_score)\n",
    "    if debug:\n",
    "        end_time = process_time()\n",
    "        print(\"Time to find best match from \", len(plots), \" films: \", end_time - start_time)\n",
    "        print(\"Average search time per film: \", (end_time-start_time)/len(plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some tests/examples\n",
    "simularity_printer(10, 250)\n",
    "simularity_printer(10, 300)\n",
    "simularity_printer(27, 72)\n",
    "simularity_printer(27, 207)\n",
    "simularity_printer(27, 270)\n",
    "\n",
    "best_match(2000)\n",
    "best_match(2500)\n",
    "best_match(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match(2915)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best matches for In This Our Life are:\n",
      "1. Diary of a Mad Black Woman \t 0.9897135738481504\n",
      "2. Night World \t 0.9886595584147576\n",
      "3. The Young Philadelphians \t 0.9881839554328727\n",
      "4. Peyton Place \t 0.9879365048790834\n",
      "5. Phone Call from a Stranger \t 0.9878915190776927\n",
      "6. Ten North Frederick \t 0.9875224143707632\n",
      "7. Devotion \t 0.9872187233170223\n",
      "8. St. Elmo's Fire \t 0.9871605721632734\n",
      "9. Red-Headed Woman \t 0.9871487847600823\n",
      "10. Chained \t 0.9871139572867368\n",
      "\n",
      "Time to find best match from  34886  films:  50.546875\n",
      "Average search time per film:  0.0014489157541707274\n"
     ]
    }
   ],
   "source": [
    "def top10_match(id):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    matches = [['BASE VALUE', 0]] # Schema: [ [title, score], [title, score]... ]\n",
    "    \n",
    "    for plot in plots:\n",
    "        if plot == plots[id]:\n",
    "            continue\n",
    "\n",
    "        score = plots[id][1].similarity(plot[1]) # Get Similarity between current movie and match movie\n",
    "\n",
    "        matches.append([plot[0], score]) # Add movie title and score to end of list\n",
    "        matches = sorted(matches, key=lambda x: x[1], reverse=True) # Sort list by scores\n",
    "        if len(matches) > 10: # Remove lowest score if length > 10\n",
    "            matches.pop()\n",
    "    \n",
    "    end_time = process_time()\n",
    "    # Print Results\n",
    "    print(\"The best matches for\", plots[id][0], \"are:\")\n",
    "    i = 1\n",
    "    for match in matches:\n",
    "        print(str(i) + \".\", match[0], \"\\t\", match[1])\n",
    "        i+=1\n",
    "\n",
    "    print(\"\\nTime to find best match from \", len(plots), \" films: \", end_time - start_time)\n",
    "    print(\"Average search time per film: \", (end_time-start_time)/len(plots))\n",
    "\n",
    "top10_match(3331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  1000  movies\n",
      "Processed  2000  movies\n",
      "Processed  3000  movies\n",
      "Processed  4000  movies\n",
      "Processed  5000  movies\n",
      "Processed  6000  movies\n",
      "Processed  7000  movies\n",
      "Processed  8000  movies\n",
      "Processed  9000  movies\n",
      "Processed  10000  movies\n",
      "Processed  11000  movies\n",
      "Processed  12000  movies\n",
      "Processed  13000  movies\n",
      "Processed  14000  movies\n",
      "Processed  15000  movies\n",
      "Processed  16000  movies\n",
      "Processed  17000  movies\n",
      "Processed  18000  movies\n",
      "Processed  19000  movies\n",
      "Processed  20000  movies\n",
      "Processed  21000  movies\n",
      "Processed  22000  movies\n",
      "Processed  23000  movies\n",
      "Processed  24000  movies\n",
      "Processed  25000  movies\n",
      "Processed  26000  movies\n",
      "Processed  27000  movies\n",
      "Processed  28000  movies\n",
      "Processed  29000  movies\n",
      "Processed  30000  movies\n",
      "Processed  31000  movies\n",
      "Processed  32000  movies\n",
      "Processed  33000  movies\n",
      "Processed  34000  movies\n",
      "Processed all  34886  movies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "import io\n",
    "\n",
    "best_matches=list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for plot in plots:\n",
    "    matches = [['BASE VALUE', 0]] # Schema: [ [title, score], [title, score]... ]\n",
    "    \n",
    "    for compplot in plots:\n",
    "        if compplot == plot:\n",
    "            continue\n",
    "\n",
    "        score = plot[1].similarity(compplot[1]) # Get Similarity between current movie and match movie\n",
    "\n",
    "        matches.append([compplot[0], score]) # Add movie title and score to end of list\n",
    "        matches = sorted(matches, key=lambda x: x[1], reverse=True) # Sort list by scores\n",
    "        if len(matches) > 10: # Remove lowest score if length > 10\n",
    "            matches.pop()\n",
    "    \n",
    "    best_matches.append((plot[0],matches))\n",
    "    count = count + 1\n",
    "    if(count % 1000 == 0):\n",
    "        print(\"Processed \", count, \" movies\")\n",
    "print (\"Processed all \", count, \" movies\")\n",
    "\n",
    "f = open('toptennostopwords.dat','wb')\n",
    "pickle.dump(best_matches,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to Plots of plot without stopwords and punctuation - plots[id][2]\n",
    "cleaned_plots = []\n",
    "counter = 0\n",
    "\n",
    "for plot in plots:\n",
    "    if debug:\n",
    "        counter+=1\n",
    "        if (counter % 100 == 0):\n",
    "            print(counter, \"movie plots cleaned\")\n",
    "    token_list = []\n",
    "    for token in plot[1]:\n",
    "        if not nlp.vocab[token.text].is_stop and not nlp.vocab[token.text].is_punct and not token.text == \"\\n\":\n",
    "            token_list.append(token.text)\n",
    "\n",
    "    cleaned_plots.append([plot[0], nlp(' '.join(str(token) for token in token_list))])\n",
    "\n",
    "print(cleaned_plots[100][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_match_cleaned_plots(id):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    matches = [['BASE VALUE', 0]] # Schema: [ [title, score], [title, score]... ]\n",
    "    \n",
    "    for plot in cleaned_plots:\n",
    "        if plot == cleaned_plots[id]:\n",
    "            continue\n",
    "\n",
    "        score = cleaned_plots[id][1].similarity(plot[1]) # Get Similarity between current movie and match movie\n",
    "\n",
    "        matches.append([plot[0], score]) # Add movie title and score to end of list\n",
    "        matches = sorted(matches, key=lambda x: x[1], reverse=True) # Sort list by scores\n",
    "        if len(matches) > 10: # Remove lowest score if length > 10\n",
    "            matches.pop()\n",
    "    \n",
    "    end_time = process_time()\n",
    "    # Print Results\n",
    "    print(\"The best matches for\", cleaned_plots[id][0], \"are:\")\n",
    "    i = 1\n",
    "    for match in matches:\n",
    "        print(str(i) + \".\", match[0], \"\\t\", match[1])\n",
    "        i+=1\n",
    "\n",
    "    print(\"\\nTime to find best match from \", len(cleaned_plots), \" films: \", end_time - start_time)\n",
    "    print(\"Average search time per film: \", (end_time-start_time)/len(cleaned_plots))\n",
    "\n",
    "top10_match_cleaned_plots(3331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take Plots[10] and Plots[100] and remove the stopwords\n",
    "# TODO: Find more efficient way to remove stopwords\n",
    "# TODO: Remove stopwords from ALL plots\n",
    "\n",
    "# Create blank token list\n",
    "first_token_list = []\n",
    "# Tokenize plots - add each word to list\n",
    "for token in plots[10][1]:\n",
    "    first_token_list.append(token.text)\n",
    "\n",
    "# Create a new string combining all the tokens (words) that are not stopwords\n",
    "first_filtered_plot_str = ' '.join([str(token) for token in first_token_list if not nlp.vocab[token].is_stop])\n",
    "\n",
    "# Repeat stopword removal for second plot\n",
    "second_token_list = []\n",
    "for token in plots[100][1]:\n",
    "    second_token_list.append(token.text)      \n",
    "\n",
    "second_filtered_plot_str = ' '.join([str(token) for token in second_token_list if not nlp.vocab[token].is_stop])\n",
    "\n",
    "# Print standard simularity with stopwords\n",
    "simularity_printer(10, 100)\n",
    "\n",
    "# Print Filtered Stopword similarity to test stopword removal impact\n",
    "print(\"Filtered Stopwords: \", nlp(first_filtered_plot_str).similarity(nlp(second_filtered_plot_str)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments with bag of words for keyword group mining\n",
    "from bitarray import bitarray\n",
    "from apyori import apriori\n",
    "\n",
    "\n",
    "lexicon = {}\n",
    "for plot in plots:\n",
    "    for token in plot[1]:\n",
    "        if not nlp.vocab[token.text].is_stop and not nlp.vocab[token.text].is_punct and not token.text == \"\\n\":\n",
    "            if token.lemma_ not in lexicon.keys():\n",
    "                lexicon[token.lemma_] = 1\n",
    "            else:\n",
    "                lexicon[token.lemma_] += 1\n",
    "\n",
    "print(list(lexicon)[0:25])\n",
    "print(len(lexicon))\n",
    "\n",
    "plot_word_bags = []\n",
    "plot_word_lists = []\n",
    "\n",
    "for plot in plots:\n",
    "    plot_words = set()\n",
    "    plot_words_list = []\n",
    "    for token in plot[1]:\n",
    "        if not nlp.vocab[token.text].is_stop and not nlp.vocab[token.text].is_punct:\n",
    "            plot_words.add(token.lemma_)\n",
    "    word_bag = bitarray()\n",
    "    for word in lexicon:\n",
    "        if word in plot_words:\n",
    "            word_bag.append(1)\n",
    "            plot_words_list.append(word)\n",
    "        else:\n",
    "            word_bag.append(0)\n",
    "    plot_word_bags.append(word_bag)\n",
    "    plot_word_lists.append(plot_words_list)\n",
    "\n",
    "plot_num = 400\n",
    "#print(plots[plot_num][0])\n",
    "#print(plot_word_bags[plot_num])\n",
    "#for i in range(0, len(lexicon)):\n",
    "#    if plot_word_bags[plot_num][i]:\n",
    "#        print(list(lexicon.keys())[i])\n",
    "\n",
    "#Ignore compact bit vectors for now, see what happens with an out of the box apriori algorithm\n",
    "results = list(apriori(plot_word_lists, min_support = .1))\n",
    "print(\"Apriori Test\")\n",
    "print(\"Total results: \", len(results))\n",
    "for result in results:\n",
    "    print(result.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportSorter(e):\n",
    "  return e.support\n",
    "\n",
    "results.sort( key=supportSorter)\n",
    "\n",
    "for result in results:\n",
    "  print(result.items, result.support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000  ratings processed\n",
      "2000000  ratings processed\n",
      "3000000  ratings processed\n",
      "4000000  ratings processed\n",
      "5000000  ratings processed\n",
      "6000000  ratings processed\n",
      "7000000  ratings processed\n",
      "8000000  ratings processed\n",
      "9000000  ratings processed\n",
      "10000000  ratings processed\n",
      "11000000  ratings processed\n",
      "12000000  ratings processed\n",
      "13000000  ratings processed\n",
      "14000000  ratings processed\n",
      "15000000  ratings processed\n",
      "16000000  ratings processed\n",
      "17000000  ratings processed\n",
      "18000000  ratings processed\n",
      "19000000  ratings processed\n",
      "20000000  ratings processed\n",
      "21000000  ratings processed\n",
      "22000000  ratings processed\n",
      "23000000  ratings processed\n",
      "24000000  ratings processed\n",
      "25000000  ratings processed\n",
      "26000000  ratings processed\n",
      "Finished processing all  26024289  ratings\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "rated_movies = set()\n",
    "rated_movies_dict = {}\n",
    "#movie_ratings = set()\n",
    "movie_viewers = defaultdict(lambda: list())\n",
    "viewer_movies = defaultdict(lambda: list())\n",
    "\n",
    "\n",
    "with open(\"movies_metadata.csv\", 'r', encoding='utf-8') as movie_meta_data:\n",
    "    movie_reader = csv.reader(movie_meta_data)\n",
    "    next(movie_reader, None)  # Skip the csv header row\n",
    "\n",
    "    for row in movie_reader:\n",
    "        #print(row[5])\n",
    "        #print(row[8])\n",
    "        rated_movies.add((row[5],row[8]))\n",
    "        rated_movies_dict[row[8]]=row[5]\n",
    "\n",
    "#userId,movieId,rating,timestamp\n",
    "with open(\"ratings.csv\", 'r', encoding='utf-8') as movie_ratings_data:\n",
    "    ratings_reader = csv.reader(movie_ratings_data)\n",
    "    next(ratings_reader, None)\n",
    "    count = 0\n",
    "\n",
    "    for row in ratings_reader:\n",
    "        count = count + 1\n",
    "        if(count % 1000000 == 0):\n",
    "            print(count, \" ratings processed\")\n",
    "       # movie_ratings.add((row[0],row[1],row[2]))\n",
    "        movie_viewers[row[1]].append((row[0],row[1],float(row[2])))\n",
    "       # viewer_movies[row[0]].add((row[0],row[1],row[2]))\n",
    "    print(\"Finished processing all \", count, \" ratings\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a movie return the count of both how many people who liked the first movie liked the second movie\n",
    "# and how many people who liked the first movie didn't like the second movie\n",
    "def rating_comparison(base_movie_name, comparison_movie_name):\n",
    "    if(base_movie_name not in rated_movies_dict):\n",
    "        return (0, 0)\n",
    "    if(comparison_movie_name not in rated_movies_dict):\n",
    "        return (0, 0)\n",
    "    base_movie_id = rated_movies_dict[base_movie_name]\n",
    "    comparison_movie_id=rated_movies_dict[comparison_movie_name]\n",
    "    #print(base_movie_id)\n",
    "    #print(comparison_movie_id)\n",
    "    good_match = 0\n",
    "    bad_match = 0\n",
    "    for base_viewer in movie_viewers[base_movie_id]:\n",
    "        if(base_viewer[2] >= 4):\n",
    "            for comparison_viewer in movie_viewers[comparison_movie_id]:\n",
    "                if(comparison_viewer[0] == base_viewer[0]):\n",
    "                    if(comparison_viewer[2] >= 4):\n",
    "                        good_match = good_match + 1\n",
    "                    elif(comparison_viewer[2] <= 3):\n",
    "                        bad_match = bad_match +1\n",
    "                    break\n",
    "    #print(\"Good Matches: \", good_match)\n",
    "    #print(\"Bad Matches: \", bad_match)\n",
    "    return(good_match, bad_match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 22)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_comparison(\"The Matrix\", \"The Fifth Element\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500  movies processed\n",
      "1000  movies processed\n",
      "1500  movies processed\n",
      "2000  movies processed\n",
      "2500  movies processed\n",
      "3000  movies processed\n",
      "3500  movies processed\n",
      "4000  movies processed\n",
      "4500  movies processed\n",
      "5000  movies processed\n",
      "5500  movies processed\n",
      "6000  movies processed\n",
      "6500  movies processed\n",
      "7000  movies processed\n",
      "7500  movies processed\n",
      "8000  movies processed\n",
      "8500  movies processed\n",
      "9000  movies processed\n",
      "9500  movies processed\n",
      "10000  movies processed\n",
      "10500  movies processed\n",
      "11000  movies processed\n",
      "11500  movies processed\n",
      "12000  movies processed\n",
      "12500  movies processed\n",
      "13000  movies processed\n",
      "13500  movies processed\n",
      "14000  movies processed\n",
      "14500  movies processed\n",
      "15000  movies processed\n",
      "15500  movies processed\n",
      "16000  movies processed\n",
      "16500  movies processed\n",
      "17000  movies processed\n",
      "17500  movies processed\n",
      "18000  movies processed\n",
      "18500  movies processed\n",
      "19000  movies processed\n",
      "19500  movies processed\n",
      "20000  movies processed\n",
      "20500  movies processed\n",
      "21000  movies processed\n",
      "21500  movies processed\n",
      "22000  movies processed\n",
      "22500  movies processed\n",
      "23000  movies processed\n",
      "23500  movies processed\n",
      "24000  movies processed\n",
      "24500  movies processed\n",
      "25000  movies processed\n",
      "25500  movies processed\n",
      "26000  movies processed\n",
      "26500  movies processed\n",
      "27000  movies processed\n",
      "27500  movies processed\n",
      "28000  movies processed\n",
      "28500  movies processed\n",
      "29000  movies processed\n",
      "29500  movies processed\n",
      "30000  movies processed\n",
      "30500  movies processed\n",
      "31000  movies processed\n",
      "31500  movies processed\n",
      "32000  movies processed\n",
      "32500  movies processed\n",
      "33000  movies processed\n",
      "33500  movies processed\n",
      "34000  movies processed\n",
      "34500  movies processed\n",
      "Total Good / Badd matches\n",
      "485147\n",
      "125259\n"
     ]
    }
   ],
   "source": [
    "total_good_matches = 0\n",
    "total_bad_matches = 0\n",
    "count = 0\n",
    "for movie_matches in best_matches:\n",
    "    for match in movie_matches[1]:\n",
    "        temp_score = rating_comparison(movie_matches[0], match[0])\n",
    "        total_good_matches = total_good_matches + temp_score[0]\n",
    "        total_bad_matches = total_bad_matches + temp_score[1]\n",
    "    count = count + 1\n",
    "    if(count%500 == 0):\n",
    "        print(count , \" movies processed\")\n",
    "print(\"Total Good / Bad matches\")\n",
    "print(total_good_matches)\n",
    "print(total_bad_matches)\n",
    "#rating_comparison(best_matches[test_movie_id][0],best_matches[test_movie_id][1][3][0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
