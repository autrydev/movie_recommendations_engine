{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries and Packages\n",
    "import sys  \n",
    "import csv\n",
    "import spacy\n",
    "from time import process_time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Set global variables\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if debug:\n",
    "    start_time = process_time()\n",
    "\n",
    "# Set the appropriate input file\n",
    "# movie_plots_import_file = \"wiki_movie_plots_sample.csv\" # 3248 Movies\n",
    "movie_plots_import_file = \"wiki_movie_plots_deduped.csv\" #34,892 Movies\n",
    "\n",
    "\n",
    "# Setup the plots array\n",
    "plots = []\n",
    "\n",
    "# Open import file and export all plots and titles into plots[]\n",
    "with open(movie_plots_import_file, 'r', encoding='utf-8') as movie_plot_csv_file:\n",
    "    reader = csv.reader(movie_plot_csv_file)\n",
    "    next(reader, None)  # Skip the csv header row\n",
    "    count=0\n",
    "\n",
    "    for row in reader:\n",
    "        plots.append((row[1],nlp(row[7])))\n",
    "        count = count + 1\n",
    "        if(count % 500 == 0):\n",
    "            print(count, \" movies processed\")\n",
    "            \n",
    "print(\"Processing Done\")\n",
    "\n",
    "if debug:\n",
    "    end_time = process_time()\n",
    "    print(\"Time to process \", count, \" records: \", end_time - start_time)\n",
    "    print(\"Average processing timer per record: \", (end_time- start_time)/count)\n",
    "\n",
    "# Print example when finished\n",
    "print(plots[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic similarity function\n",
    "def simularity_printer(firstId, secondId):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    print(\"Similarity of \", plots[firstId][0], \" and \", plots[secondId][0], \":\", plots[firstId][1].similarity(plots[secondId][1]))\n",
    "    if debug:\n",
    "        end_time = process_time()\n",
    "        print(\"Simularity calculation runtime \", end_time - start_time)\n",
    "\n",
    "# Define generic 'best match' function\n",
    "def best_match(id):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    best_score = 0\n",
    "    best_match = \"None\"\n",
    "    for plot in plots:\n",
    "        if plot == plots[id]:\n",
    "            continue\n",
    "        score = plots[id][1].similarity(plot[1])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = plot[0]\n",
    "    print(\"Best match for \",plots[id][0],\" was: \",best_match,\" with score of \", best_score)\n",
    "    if debug:\n",
    "        end_time = process_time()\n",
    "        print(\"Time to find best match from \", len(plots), \" films: \", end_time - start_time)\n",
    "        print(\"Average search time per film: \", (end_time-start_time)/len(plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some tests/examples\n",
    "simularity_printer(10, 250)\n",
    "simularity_printer(10, 300)\n",
    "simularity_printer(27, 72)\n",
    "simularity_printer(27, 207)\n",
    "simularity_printer(27, 270)\n",
    "\n",
    "best_match(2000)\n",
    "best_match(2500)\n",
    "best_match(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match(2915)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_match(id):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    matches = [['BASE VALUE', 0]] # Schema: [ [title, score], [title, score]... ]\n",
    "    \n",
    "    for plot in plots:\n",
    "        if plot == plots[id]:\n",
    "            continue\n",
    "\n",
    "        score = plots[id][1].similarity(plot[1]) # Get Similarity between current movie and match movie\n",
    "\n",
    "        matches.append([plot[0], score]) # Add movie title and score to end of list\n",
    "        matches = sorted(matches, key=lambda x: x[1], reverse=True) # Sort list by scores\n",
    "        if len(matches) > 10: # Remove lowest score if length > 10\n",
    "            matches.pop()\n",
    "    \n",
    "    end_time = process_time()\n",
    "    # Print Results\n",
    "    print(\"The best matches for\", plots[id][0], \"are:\")\n",
    "    i = 1\n",
    "    for match in matches:\n",
    "        print(str(i) + \".\", match[0], \"\\t\", match[1])\n",
    "        i+=1\n",
    "\n",
    "    print(\"\\nTime to find best match from \", len(plots), \" films: \", end_time - start_time)\n",
    "    print(\"Average search time per film: \", (end_time-start_time)/len(plots))\n",
    "\n",
    "top10_match(3331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to Plots of plot without stopwords and punctuation - plots[id][2]\n",
    "cleaned_plots = []\n",
    "counter = 0\n",
    "\n",
    "for plot in plots:\n",
    "    if debug:\n",
    "        counter+=1\n",
    "        if (counter % 100 == 0):\n",
    "            print(counter, \"movie plots cleaned\")\n",
    "    token_list = []\n",
    "    for token in plot[1]:\n",
    "        if not nlp.vocab[token.text].is_stop and not nlp.vocab[token.text].is_punct and not token.text == \"\\n\":\n",
    "            token_list.append(token.text)\n",
    "\n",
    "    cleaned_plots.append([plot[0], nlp(' '.join(str(token) for token in token_list))])\n",
    "\n",
    "print(cleaned_plots[100][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_match_cleaned_plots(id):\n",
    "    if debug:\n",
    "        start_time = process_time()\n",
    "    matches = [['BASE VALUE', 0]] # Schema: [ [title, score], [title, score]... ]\n",
    "    \n",
    "    for plot in cleaned_plots:\n",
    "        if plot == cleaned_plots[id]:\n",
    "            continue\n",
    "\n",
    "        score = cleaned_plots[id][1].similarity(plot[1]) # Get Similarity between current movie and match movie\n",
    "\n",
    "        matches.append([plot[0], score]) # Add movie title and score to end of list\n",
    "        matches = sorted(matches, key=lambda x: x[1], reverse=True) # Sort list by scores\n",
    "        if len(matches) > 10: # Remove lowest score if length > 10\n",
    "            matches.pop()\n",
    "    \n",
    "    end_time = process_time()\n",
    "    # Print Results\n",
    "    print(\"The best matches for\", cleaned_plots[id][0], \"are:\")\n",
    "    i = 1\n",
    "    for match in matches:\n",
    "        print(str(i) + \".\", match[0], \"\\t\", match[1])\n",
    "        i+=1\n",
    "\n",
    "    print(\"\\nTime to find best match from \", len(cleaned_plots), \" films: \", end_time - start_time)\n",
    "    print(\"Average search time per film: \", (end_time-start_time)/len(cleaned_plots))\n",
    "\n",
    "top10_match_cleaned_plots(3331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take Plots[10] and Plots[100] and remove the stopwords\n",
    "# TODO: Find more efficient way to remove stopwords\n",
    "# TODO: Remove stopwords from ALL plots\n",
    "\n",
    "# Create blank token list\n",
    "first_token_list = []\n",
    "# Tokenize plots - add each word to list\n",
    "for token in plots[10][1]:\n",
    "    first_token_list.append(token.text)\n",
    "\n",
    "# Create a new string combining all the tokens (words) that are not stopwords\n",
    "first_filtered_plot_str = ' '.join([str(token) for token in first_token_list if not nlp.vocab[token].is_stop])\n",
    "\n",
    "# Repeat stopword removal for second plot\n",
    "second_token_list = []\n",
    "for token in plots[100][1]:\n",
    "    second_token_list.append(token.text)      \n",
    "\n",
    "second_filtered_plot_str = ' '.join([str(token) for token in second_token_list if not nlp.vocab[token].is_stop])\n",
    "\n",
    "# Print standard simularity with stopwords\n",
    "simularity_printer(10, 100)\n",
    "\n",
    "# Print Filtered Stopword similarity to test stopword removal impact\n",
    "print(\"Filtered Stopwords: \", nlp(first_filtered_plot_str).similarity(nlp(second_filtered_plot_str)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments with bag of words for keyword group mining\n",
    "from bitarray import bitarray\n",
    "from apyori import apriori\n",
    "\n",
    "\n",
    "lexicon = {}\n",
    "for plot in plots:\n",
    "    for token in plot[1]:\n",
    "        if not nlp.vocab[token.text].is_stop and not nlp.vocab[token.text].is_punct and not token.text == \"\\n\":\n",
    "            if token.lemma_ not in lexicon.keys():\n",
    "                lexicon[token.lemma_] = 1\n",
    "            else:\n",
    "                lexicon[token.lemma_] += 1\n",
    "\n",
    "print(list(lexicon)[0:25])\n",
    "print(len(lexicon))\n",
    "\n",
    "plot_word_bags = []\n",
    "plot_word_lists = []\n",
    "\n",
    "for plot in plots:\n",
    "    plot_words = set()\n",
    "    plot_words_list = []\n",
    "    for token in plot[1]:\n",
    "        if not nlp.vocab[token.text].is_stop and not nlp.vocab[token.text].is_punct:\n",
    "            plot_words.add(token.lemma_)\n",
    "    word_bag = bitarray()\n",
    "    for word in lexicon:\n",
    "        if word in plot_words:\n",
    "            word_bag.append(1)\n",
    "            plot_words_list.append(word)\n",
    "        else:\n",
    "            word_bag.append(0)\n",
    "    plot_word_bags.append(word_bag)\n",
    "    plot_word_lists.append(plot_words_list)\n",
    "\n",
    "plot_num = 400\n",
    "#print(plots[plot_num][0])\n",
    "#print(plot_word_bags[plot_num])\n",
    "#for i in range(0, len(lexicon)):\n",
    "#    if plot_word_bags[plot_num][i]:\n",
    "#        print(list(lexicon.keys())[i])\n",
    "\n",
    "#Ignore compact bit vectors for now, see what happens with an out of the box apriori algorithm\n",
    "results = list(apriori(plot_word_lists, min_support = .1))\n",
    "print(\"Apriori Test\")\n",
    "print(\"Total results: \", len(results))\n",
    "for result in results:\n",
    "    print(result.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportSorter(e):\n",
    "  return e.support\n",
    "\n",
    "results.sort( key=supportSorter)\n",
    "\n",
    "for result in results:\n",
    "  print(result.items, result.support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "rated_movies = set()\n",
    "rated_movies_dict = {}\n",
    "#movie_ratings = set()\n",
    "movie_viewers = defaultdict(lambda: list())\n",
    "viewer_movies = defaultdict(lambda: list())\n",
    "\n",
    "\n",
    "with open(\"movies_metadata.csv\", 'r', encoding='utf-8') as movie_meta_data:\n",
    "    movie_reader = csv.reader(movie_meta_data)\n",
    "    next(movie_reader, None)  # Skip the csv header row\n",
    "\n",
    "    for row in movie_reader:\n",
    "        #print(row[5])\n",
    "        #print(row[8])\n",
    "        rated_movies.add((row[5],row[8]))\n",
    "        rated_movies_dict[row[8]]=row[5]\n",
    "\n",
    "#userId,movieId,rating,timestamp\n",
    "with open(\"ratings.csv\", 'r', encoding='utf-8') as movie_ratings_data:\n",
    "    ratings_reader = csv.reader(movie_ratings_data)\n",
    "    next(ratings_reader, None)\n",
    "    count = 0\n",
    "\n",
    "    for row in ratings_reader:\n",
    "        count = count + 1\n",
    "        if(count % 1000000 == 0):\n",
    "            print(count, \" ratings processed\")\n",
    "       # movie_ratings.add((row[0],row[1],row[2]))\n",
    "        movie_viewers[row[1]].append((row[0],row[1],float(row[2])))\n",
    "       # viewer_movies[row[0]].add((row[0],row[1],row[2]))\n",
    "    print(\"Finished processing all \", count, \" ratings\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a movie return the count of both how many people who liked the first movie liked the second movie\n",
    "# and how many people who liked the first movie didn't like the second movie\n",
    "def rating_comparison(base_movie_name, comparison_movie_name):\n",
    "    base_movie_id = rated_movies_dict[base_movie_name]\n",
    "    comparison_movie_id=rated_movies_dict[comparison_movie_name]\n",
    "    print(base_movie_id)\n",
    "    print(comparison_movie_id)\n",
    "    good_match = 0\n",
    "    bad_match = 0\n",
    "    for base_viewer in movie_viewers[base_movie_id]:\n",
    "        if(base_viewer[2] >= 4):\n",
    "            for comparison_viewer in movie_viewers[comparison_movie_id]:\n",
    "                if(comparison_viewer[0] == base_viewer[0]):\n",
    "                    if(comparison_viewer[2] >= 4):\n",
    "                        good_match = good_match + 1\n",
    "                    elif(comparison_viewer[2] <= 3):\n",
    "                        bad_match = bad_match +1\n",
    "                    break\n",
    "    print(\"Good Matches: \", good_match)\n",
    "    print(\"Bad Matches: \", bad_match)\n",
    "    return(good_match, bad_match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_comparison(\"The Matrix\", \"The Fifth Element\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
